"""
ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®æ¦‚è¦:
Akumaï¼ˆakuma.aiï¼‰ã®ã‚¸ãƒ§ãƒ–è©³ç´°ãƒšãƒ¼ã‚¸ã‹ã‚‰ç”»åƒã‚„èª¬æ˜æ–‡ï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼‰ã‚’è‡ªå‹•å–å¾—ã™ã‚‹ãŸã‚ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§ã™ã€‚
äº‹å‰ã«state.jsonã‚’ç”Ÿæˆã—ã¦ãŠãã“ã¨ã§ã€èªè¨¼çŠ¶æ…‹ã‚’å¼•ãç¶™ã„ã§ãƒ‡ãƒ¼ã‚¿å–å¾—ãŒå¯èƒ½ã§ã™ã€‚
"""

import os
import json
import time
from dotenv import load_dotenv
from bs4 import BeautifulSoup
from playwright.sync_api import sync_playwright
import requests
from lib.get_playwright_context import get_playwright_context
from urllib.parse import urlparse, parse_qs, urljoin, unquote

load_dotenv()

def fetch_image_url_with_saved_state(job_url: str, state_file: str) -> str:
    """
    ï¼ˆéæ¨å¥¨ï¼‰Akumaã®ã‚¸ãƒ§ãƒ–ãƒšãƒ¼ã‚¸ã‹ã‚‰__NEXT_DATA__ã‚’ä½¿ã£ã¦ç”»åƒURLã‚’å–å¾—ã—ã¾ã™ã€‚
    Args:
        job_url: å–å¾—å¯¾è±¡ã®Akumaã‚¸ãƒ§ãƒ–ãƒšãƒ¼ã‚¸ã®URL
        state_file: èªè¨¼æƒ…å ±ï¼ˆstate.jsonï¼‰ã®ãƒ‘ã‚¹
    Returns:
        ç”»åƒã®URLï¼ˆæ–‡å­—åˆ—ï¼‰
    Note:
        ç¾åœ¨ã®Akumaã®ä»•æ§˜ã§ã¯__NEXT_DATA__ãŒå­˜åœ¨ã—ãªã„å ´åˆãŒã‚ã‚‹ãŸã‚ã€
        fetch_akuma_job_infoã®åˆ©ç”¨ã‚’æ¨å¥¨ã—ã¾ã™ã€‚
    """
    with sync_playwright() as p:
        # â‘  èªè¨¼æƒ…å ±ã‚’å¼•ãç¶™ã„ã§ãƒ–ãƒ©ã‚¦ã‚¶èµ·å‹•
        browser = p.chromium.launch(headless=False)
        context = browser.new_context(storage_state=state_file)
        page = context.new_page()

        # â‘¡ ã‚¸ãƒ§ãƒ–ãƒšãƒ¼ã‚¸ã¸ç§»å‹•
        print(f"ğŸ“„ ã‚¸ãƒ§ãƒ–ãƒšãƒ¼ã‚¸ã‚’é–‹ã: {job_url}")
        page.goto(job_url, wait_until="networkidle")
        time.sleep(2)

        # â‘¢ HTML å–å¾—â†’__NEXT_DATA__ æŠ½å‡º
        content = page.content()
        soup = BeautifulSoup(content, "html.parser")
        script = soup.find("script", id="__NEXT_DATA__")
        if not script:
            raise RuntimeError("__NEXT_DATA__ ãŒè¦‹ã¤ã‹ã‚‰ãªã„")

        data = json.loads(script.string)
        props = data["props"]["pageProps"]
        image_url = props.get("image", {}).get("url")
        if not image_url:
            raise RuntimeError("ç”»åƒ URL ãŒå–å¾—ã§ããªã‹ã£ãŸ")

        browser.close()
        return image_url

def fetch_akuma_job_info(job_url, output_dir=None, state_file=None):
    """
    Akumaã®ã‚¸ãƒ§ãƒ–è©³ç´°ãƒšãƒ¼ã‚¸ã‹ã‚‰ç”»åƒã¨èª¬æ˜æ–‡ã‚’å–å¾—ã—ã€ç”»åƒã‚’ä¿å­˜ã—ã¾ã™ã€‚
    Args:
        job_url: å–å¾—å¯¾è±¡ã®Akumaã‚¸ãƒ§ãƒ–ãƒšãƒ¼ã‚¸ã®URL
        output_dir: ç”»åƒä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆNoneã®å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨ï¼‰
        state_file: èªè¨¼æƒ…å ±ï¼ˆstate.jsonï¼‰ã®ãƒ‘ã‚¹
    Returns:
        ãªã—
    """
    from constant import OUTPUT_DIR
    if output_dir is None:
        output_dir = OUTPUT_DIR
    
    p, browser, context = get_playwright_context(state_file=state_file, headless=True)
    page = context.new_page()
    page.goto(job_url, wait_until="networkidle")

    # ç”»åƒURLå–å¾—
    img_selector = 'main img'
    page.wait_for_selector(img_selector)
    img_url = page.get_attribute(img_selector, "src")
    if img_url.startswith("/_next/image"):
        img_url_full = "https://akuma.ai" + img_url
        parsed = urlparse(img_url_full)
        qs = parse_qs(parsed.query)
        if "url" in qs:
            img_url = unquote(qs["url"][0])
        else:
            img_url = urljoin("https://akuma.ai", img_url)
    elif img_url.startswith("/"):
        img_url = urljoin("https://akuma.ai", img_url)

    # èª¬æ˜æ–‡å–å¾—
    desc_selector = 'main .text-sm'
    page.wait_for_selector(desc_selector)
    description = page.inner_text(desc_selector).strip()

    # descriptionãŒç©ºã®å ´åˆã€mainå†…ã®å…¨ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æŠ½å‡ºã‚’è©¦ã¿ã‚‹
    if not description:
        main_text = page.inner_text("main")
        lines = [line.strip() for line in main_text.splitlines() if line.strip()]
        for i, line in enumerate(lines):
            if line == "Publish" and i + 1 < len(lines):
                description = lines[i + 1]
                break

    print("ç”»åƒURL:", img_url)
    print("èª¬æ˜æ–‡:", description)

    # ç”»åƒãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    if img_url:
        os.makedirs(output_dir, exist_ok=True)
        img_data = requests.get(img_url).content
        
        # ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ
        from datetime import datetime
        import re
        
        # URLã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«æ‹¡å¼µå­ã‚’å–å¾—
        url_path = urlparse(img_url).path
        file_extension = os.path.splitext(url_path)[1]
        if not file_extension:
            file_extension = ".png"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæ‹¡å¼µå­
        
        # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ãƒ™ãƒ¼ã‚¹ã®ãƒ•ã‚¡ã‚¤ãƒ«å
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # URLã‹ã‚‰ã‚¸ãƒ§ãƒ–IDã‚’æŠ½å‡ºï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        job_id = ""
        if "jobs/" in job_url:
            job_match = re.search(r'jobs/([^/?]+)', job_url)
            if job_match:
                job_id = f"_{job_match.group(1)[:8]}"  # æœ€åˆã®8æ–‡å­—
        
        filename = f"akuma_image{job_id}_{timestamp}{file_extension}"
        img_path = os.path.join(output_dir, filename)
        
        with open(img_path, "wb") as f:
            f.write(img_data)
        print(f"ç”»åƒã‚’ä¿å­˜ã—ã¾ã—ãŸ: {img_path}")

    browser.close()
    p.stop()

if __name__ == "__main__":
    # äº‹å‰ã«æ‰‹å‹•ãƒ­ã‚°ã‚¤ãƒ³ã—ã¦ state.json ã‚’ç”Ÿæˆã—ã¦ãŠãã“ã¨
    # source venv/bin/activate
    # python scraping.py
    from constant import AUTH_FILE_NAME, OUTPUT_DIR
    STATE_FILE = os.path.join(os.path.dirname(__file__), AUTH_FILE_NAME)
    fetch_akuma_job_info(os.getenv("AKUMA_JOB_URL"), state_file=STATE_FILE)
